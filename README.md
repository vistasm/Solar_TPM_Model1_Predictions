# Solar TPM Model 1 — Live Prediction Repository

## Daily Solar Flare Predictions with Immutable Timestamp Evidence

This repository contains **daily solar flare predictions** generated by an automated machine learning pipeline. Every prediction is committed to this branch-protected repository **before** the forecast window opens, providing cryptographically signed, tamper-evident evidence of prediction timing.

**Model:** Solar TPM Model 1 ("Sniper") — a high-precision, low-false-positive-rate prediction system optimized for accurate alerts over alert volume.

**Live since:** February 18, 2026

---

## How It Works

### Prediction Tiers

Each daily forecast covers three flare severity levels:

| Tier | Flare Class | Description | Use Case |
|------|-------------|-------------|----------|
| **M+** | ≥ M1.0 | All M-class and X-class flares | Satellite operations, aviation |
| **M5+** | ≥ M5.0 | Strong M-class and all X-class | Insurance triggers, power grid alerts |
| **X+** | ≥ X1.0 | Extreme flares only | Critical infrastructure protection |

### Prediction Window

Each forecast covers a rolling 24-hour window aligned with publication time:

```
Publication:  ~06:00 UTC (Day D)
Window:       06:00 UTC (Day D+1) → 06:00 UTC (Day D+2)
```

Every predicted flare falls **after** publication — no retroactive predictions. This is enforced by the Option C temporal contract used in both training and inference.

### Daily Pipeline

The pipeline runs automatically at **05:50 UTC** every day on a dedicated Google Cloud VM:

1. **Fetch** Near Real-Time (NRT) SHARP magnetogram data from JSOC (Stanford)
2. **Fetch** flare catalog from NASA DONKI for persistence features
3. **Score** all active regions on the solar disk across 3 tiers
4. **Publish** prediction JSON to this repository with a signed commit
5. **Verify** past predictions against observed flares (self-healing backfill)
6. **Report** rolling performance metrics and data freshness diagnostics

### Monthly Retrain

On the 1st of each month at **05:00 UTC**, the model retrains on an expanding window of historical data starting from 2011. Retrain uses definitive (science-quality) SHARP data and the full DONKI flare catalog. A new model deploys only if all 6 validation checkpoints pass; otherwise the previous model is retained.

---

## Repository Structure

```
Solar_TPM_Model1_Predictions/
├── README.md
├── calibration/                # Updated monthly after retrain
│   ├── training_report.json    # Training metadata, artifact hashes, validation results
│   ├── M_plus/                 # M+ tier calibration artifacts
│   │   ├── threshold_config.json
│   │   ├── imputation_medians.json
│   │   └── ...
│   ├── M5_plus/                # M5+ tier calibration artifacts
│   └── X_plus/                 # X+ tier calibration artifacts
├── predictions/                # Updated daily
│   └── 2026/
│       ├── 2026-02-18.json
│       ├── 2026-02-19.json
│       └── ...
└── aggregate/                  # Updated daily
    ├── performance_report.json       # Rolling 7/30/90-day performance metrics
    └── freshness_impact_report.md    # Data freshness monitoring report
```

---

## Prediction JSON Schema

Each daily prediction file contains:

```json
{
  "prediction_date": "2026-02-18",
  "target_date": "2026-02-19",
  "status": "SUCCESS",
  "prediction_window": {
    "start": "2026-02-19T06:00:00Z",
    "end": "2026-02-20T06:00:00Z",
    "type": "Option C (rolling 06:00 UTC)"
  },
  "model_name": "Model 1 (Sniper)",
  "model_version": "v3.0.17",
  "model_training_date": "2026-02-18",
  "data_source": "NRT",
  "environment": {
    "inference_code_hash": "git:d2e6960",
    "python_version": "3.12.3",
    "step0_artifact_check": "PASSED"
  },
  "gap_repair_stats": { "..." },
  "nrt_prefilter": { "..." },
  "persistence_status": {
    "strategy": "dynamic_best_available",
    "donki_effective_lag_hours": 54.56,
    "effective_same_ar_coverage_hours": 0,
    "lag_bucket": "DEGRADED",
    "same_ar_flares_found": { "14373": 0, "14374": 0, "..." },
    "backtest_lag_guarantee_hours": 48
  },
  "tiers": {
    "M_plus": {
      "tier": "M+",
      "global_alert": false,
      "n_ar_alerts": 0,
      "n_patches_scored": 17,
      "max_probability": 0.6911,
      "mean_probability": 0.0827,
      "ar_alerts": []
    },
    "M5_plus": { "..." },
    "X_plus": { "..." }
  }
}
```

### Key Fields

| Field | Description |
|-------|-------------|
| `prediction_date` | Date features were observed (Day D) |
| `target_date` | Date the prediction covers (Day D+1) |
| `prediction_window` | Exact 24-hour UTC window: [06:00 D+1, 06:00 D+2) |
| `status` | `SUCCESS` or `FAILED` (failure records are also committed for audit completeness) |
| `global_alert` | `true` if any active region triggered an alert for this tier |
| `n_ar_alerts` | Number of individual active regions that triggered |
| `max_probability` | Highest predicted probability across all scored regions |
| `ar_alerts` | List of alerting regions with NOAA AR number, HARPNUM, and probability |
| `persistence_status` | DONKI flare catalog freshness diagnostics and Same-AR persistence signal health |
| `lag_bucket` | `FRESH` (<24h), `OK` (24–48h), or `DEGRADED` (≥48h) — indicates DONKI catalog latency |
| `gap_repair_stats` | Data completeness audit: rows fetched, gaps detected, lookback coverage |

---

## Performance Report

`aggregate/performance_report.json` contains rolling verification metrics updated daily:

- **Windows:** 7-day, 30-day, and 90-day rolling periods
- **Per tier:** M+, M5+, X+ — each with full confusion matrix
- **Metrics:** Precision, Recall, F1, False Alarm Rate (FAR), Lift, Alert Rate, Base Rate
- **Verification:** Predictions are verified against NASA DONKI actuals with a 48-hour publication lag buffer

Performance metrics become available approximately 3 days after predictions are made (24h prediction window + 48h DONKI publication lag).

---

## Freshness Impact Report

`aggregate/freshness_impact_report.md` monitors the effect of DONKI catalog latency on prediction quality.

The model was trained under worst-case conditions (48-hour Same-AR persistence lag). In production, DONKI data may arrive faster, creating a train/serve distribution shift. This report tracks whether fresher data helps or hurts predictions, with automatic decision signals.

---

## Data Sources

| Source | Provider | Purpose | Update Frequency |
|--------|----------|---------|------------------|
| [SHARP NRT](http://jsoc.stanford.edu/) | JSOC / Stanford | Near Real-Time solar magnetogram features | ~3 hour latency |
| [SHARP Definitive](http://jsoc.stanford.edu/) | JSOC / Stanford | Science-quality magnetogram features (monthly retrain) | ~15 day latency |
| [DONKI Flare Catalog](https://kauai.ccmc.gsfc.nasa.gov/DONKI/) | NASA CCMC | Observed flare events for persistence features and verification | ~24–48 hour latency |

---

## Integrity Guarantees

This repository is designed to provide verifiable evidence that predictions were made before observed events:

| Mechanism | Purpose |
|-----------|---------|
| **Signed Git Commits (GPG)** | Proves authorship — every commit is cryptographically signed |
| **Branch Protection** | No force-push, no deletions — history is append-only and tamper-evident |
| **SHA-256 Artifact Hashes** | Model and data integrity verified before every inference run |
| **Step 0 Artifact Check** | Pre-inference validation that model on disk matches training manifest |
| **Failure Records** | Failed pipeline runs also produce a committed JSON — no silent gaps |
| **Environment Fingerprint** | Git hash + Python version recorded per prediction |
| **Gap Repair Stats** | Data completeness audit trail in every prediction JSON |

---

## Model Overview

| Property | Value |
|----------|-------|
| **Name** | Model 1 ("Sniper") |
| **Tiers** | 3 — M+ (≥M1.0), M5+ (≥M5.0), X+ (≥X1.0) |
| **Algorithms** | Logistic Regression (M+), XGBoost (M5+), Neural Network (X+) |
| **Threshold Strategy** | Fixed threshold (M+, X+), Threshold Prediction Model (M5+) |
| **Prediction Window** | [06:00 UTC D+1, 06:00 UTC D+2) — 100% actionable |
| **Training Data** | 2011-01-01 through ~15 days before retrain date |
| **Retrain** | Monthly, 1st of month, expanding walk-forward window |
| **Inference** | Daily, 05:50 UTC, NRT SHARP + DONKI persistence |
| **Validation** | Out-of-sample walk-forward with production-honest DONKI lag simulation |

---

## How to Verify Predictions

To independently verify that a prediction was made before the forecast window:

1. **Check the commit timestamp** — click any prediction file and view the Git commit date
2. **Verify GPG signature** — `git log --show-signature` confirms the signing key
3. **Compare to DONKI actuals** — query [NASA DONKI](https://kauai.ccmc.gsfc.nasa.gov/DONKI/WS/get/FLR) for flares in the prediction window
4. **Match the window** — a flare with `peakTime` between `prediction_window.start` and `prediction_window.end` counts as an event for that day

Example verification for a specific date:

```bash
# Get prediction
curl -s https://raw.githubusercontent.com/vistasm/Solar_TPM_Model1_Predictions/main/predictions/2026/2026-02-18.json | python3 -m json.tool

# Get DONKI actuals for that window
curl -s "https://kauai.ccmc.gsfc.nasa.gov/DONKI/WS/get/FLR?startDate=2026-02-19&endDate=2026-02-20" | python3 -m json.tool

# Compare: did any M-class or X-class flare occur between 06:00 Feb 19 and 06:00 Feb 20?
```

---

## Frequently Asked Questions

**Q: Why are some days "quiet" across all tiers?**
Model 1 is a high-precision "sniper" — it fires rarely but aims for accuracy. During periods of low solar activity, quiet predictions are expected and correct.

**Q: What does `lag_bucket: DEGRADED` mean?**
The DONKI flare catalog had >48h publication latency when the prediction was made. The model was trained under these conditions, so DEGRADED is the validated baseline — not a failure mode.

**Q: Why is `effective_same_ar_coverage_hours: 0`?**
When DONKI lag exceeds 36 hours, the Same-AR persistence lookback window is completely blind. The model still functions using global persistence and magnetogram features, but at its validated performance floor.

**Q: Will there be more models?**
Models 2, 3, and 4 are planned, each in a separate repository with independent infrastructure. Model 1 can be retired without affecting others.

**Q: How do I cite this work?**
Please cite as: Mishra, S. (2026). *Solar TPM Model 1 — Live Solar Flare Predictions.* GitHub repository: https://github.com/vistasm/Solar_TPM_Model1_Predictions

---

## License & Usage Rights

© 2025 Sumeet Mishra. All rights reserved.

This repository is provided for viewing, research validation, and educational review only. No other use is permitted without written permission.

| Permitted | Not Permitted |
|-----------|---------------|
| ✅ Viewing prediction data | ❌ Commercial use |
| ✅ Verifying prediction timestamps | ❌ Redistribution |
| ✅ Academic research citation | ❌ Derivative works |
| ✅ Educational review | ❌ Model replication attempts |

For licensing inquiries or commercial use requests, please open an issue in this repository.# Solar_TPM_Model1_Predictions
Solar Prediction for D+1 6 AM UTC to D+2 6 AM UTC - MODEL 1
